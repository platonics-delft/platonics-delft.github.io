<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Fine Manipulation Skills from Demonstration">
  <meta name="keywords" content="Robothon2023, Robothon, Learning, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Platonics: Learning Fine Manipulation Skills from Demonstration</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./css/index.css">
  <link rel="icon" href="./images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/franzesegiovanni/ILoSA">
            ILoSa
          </a>
          <a class="navbar-item" href="https://github.com/tud-amr/fabrics">
            Fabrics
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Platonics: Robothon 2023</h1>
          <h2 class="title is-4 publication-title">Learning fine manipulation from human demonstration</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/franzesegiovanni">Giovanni Franzese</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/MRamirez25">Mariano Ramirez</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/maxspahn">Max Spahn</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://c-salmi.github.io/">Chadi Salmi</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Delft University of Technology,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/orgs/platonics-delft/repositories"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                <a href="#software"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Quickstart</span>
                  </a>
                <a href="#hardware"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Hardware</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./assets/first_successful_run.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Platonics</span> with the first successful run
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!--Philosophy. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Philosophy</h2>
        <div class="content has-text-justified">
          <p>
            The examplary task for reducing electronic waste is a good chance
            to showcase modern solution to robotic tasks, including perception,
            control and motion planning.
            Our solution to the Robothon 2023 Challenge is built on three
            core principles:
            <ol>
              <li>Designed for non-expert users</li>
              <li>Respect humans</li>
              <li>Robustness to uncertainty</li>
            </ol> 
          </p>
          <p>
            (Uncertainty-aware) Interactive Imitation Learning (IIL) is a
            cutting-edge approach in robot manipulation that allows robots to
            learn from human demonstrations while taking into account
            uncertainty in the data and in the learned model. This approach is
            particularly useful in scenarios where the environment is dynamic
            and the robot must adapt to changes in its surroundings. It
            combines the strengths of reinforcement learning, imitation
            learning, and human guidance to teach robots complex tasks. By
            incorporating uncertainty into the learning process, the robot can
            better handle novel situations and avoid catastrophic failures. In
            contrast to traditional imitation learning, where the robot is
            simply copying the actions of the human demonstrator, IIL actively
            seeks feedback and adjusts its behavior accordingly. This two-way
            interaction between the robot and the human teacher allows for a
            more efficient learning process. IIL has shown promising results in
            a variety of tasks, including pick-and-place tasks, assembly tasks,
            and disassembly tasks. One key advantage of IIL is its ability to
            learn from human demonstrations that may be imperfect or
            incomplete. As robots become increasingly integrated into our daily
            lives, approaches like IIL will become essential for enabling safe
            and efficient interactions between humans and machines.
          </p>
        </div>
      </div>
    </div>
    <!--/ Philosopy. -->
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Learning from demonstration. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Learning from<br> Demonstration</h2>
          <p>
            Using learning from demonstration, complex skills can be easily learned
            and applied o new situations. The human demonstrator moves the robot's
            end-effector transferring implicit knowledge of the given task.
          </p>
          <video id="learning-demonstration" autoplay controls muted loop playsinline height="100%">
            <source src="./assets/insertion_demo.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Learning from demonstration. -->

      <!-- Feedback Spiraling. -->
      <div class="column">
        <h2 class="title is-3">Haptic Feedback: <br>Spiraling</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              To overcome uncertainty in the localization or the controller, 
              we use different feebacks to increase robustness and reliability.
              Using a simple <em>spiraling</em> logic helps inserting the probe.
            </p>
            <video id="spiraling-video" autoplay controls muted loop playsinline height="100%">
              <source src="./assets/spiraling.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    <!--/ Feedback Spiraling. -->
    </div>
    <div class="container is-max-desktop">

      <!-- Compliance. -->
      <div class="column">
        <h2 class="title is-3">Compliance through<br>Torque Control</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Safe coexistance of humans and robots can only be achieved
              using compliant control, i.e. a robot that is controlled using
              torque signals rather position or velocity signals. Using that 
              concept, the robot can always be held back if needed.
            </p>
            <video id="compliance-video" autoplay controls muted loop playsinline height="100%">
              <source src="./assets/compliance.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Compliance. -->

      <!-- Final Video. -->
      <div class="column">
        <h2 class="title is-3">Video Explaination</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              5 minute video presentation of our approach
            </p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/6QkUJ_hFUog" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>          </div>

        </div>
      </div>
    </div>
    <!--/ Final Video. -->
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!--Resources. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 id="hardware" class="title is-3">Resources</h2>
        <div class="content has-text-justified">
          <p>
            Our philosophy to robotics in human-shared environments requires
            the robot to have a torque interface for each motor. That means, 
            the robot must have torque sensors. One such robot is 
            <a href="https://www.franka.de/">Franka Emika Panda</a>. To
            reproduce our solution, we suggest to use the same robot, although
            other robots with that property can be used as well. The robot used
            in this challenge are
          </p>
          <p>
          <ul>
            <li>Franka Emika Panda robot aka <em>Aristotle</em></li>
            <li>The Franka Hand with two-finger gripper</li>
            <li>Gripper inlet. <a href="./assets/finger_tips.STL" download>Download STL</a></li> 

            <li><a href="https://www.coolblue.nl/product/858939/intel-nuc-kit-nuc10i7fnhn2.html">Intel NUC kit NUC10i7FNHN2</a></li>
            <li><a href="https://www.intelrealsense.com/depth-camera-d435/">Intel Realsense Camerea D435</a></li>
          </ul></p>
          <p>
            And that is it! You are ready to teach your robot any skill!
          </p>
        </div>
      </div>
    </div>
    <!--/ Resources. -->
    <!--Software. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 id="software" class="title is-3">Software Overview</h2>
        <div class="content has-text-justified">
          <p>
            Our solution to the robothon 2023 challenge is made fully
            open-source to allow the community to build on top and further
            improve our solution. All components are stored 
            <a href="https://github.com/orgs/platonics-delft">
               Platonics Delft</a>.
            You can find all required software components and links to their
            installation guides below.
            <ol>
              <li><a href="https://github.com/platonics-delft/franka_control_robothon_challenge">Franka Cartesian Impedence Controller</a></li>
              <li><a href="https://github.com/platonics-delft/robothon23_vision">robothon23_vision</a></li>
              <li><a href="https://github.com/platonics-delft/robothon23_gui">robothon23_gui</a></li>
              <li><a href="https://github.com/platonics-delft/robothon23_???">robothon_23_capabilities</a></li>
            </ol> 
          </p>
        </div>
        <h2 class="title is-3">Installation</h2>
        <div class="content has-text-justified">
          <h3><a
              href="https://github.com/platonics-delft/franka_control_robothon_challenge">Franka
              Cartesian Impedence Controller</a></h3>
          <p>
          First, we need to install <em>libfranka</em> to communicate with the robot.
          The library is publically available and can be installed in the
          <em>$HOME</em> directory using the following commands.<br>
          <code class="language-bash">
            cd $HOME<br>
            sudo apt install build-essential cmake git libpoco-dev
            libeigen3-dev<br>
            git clone --recursive https://github.com/frankaemika/libfranka<br>
            cd libfranka<br>
            mkdir build<br>
            cd build<br>
            cmake -DCMAKE_BUILD_TYPE=Release ..<br>
            cmake --build .<br>
          </code>
          </p>
          <p>
          In the next step, we install the ros-wrapper (we assume that you have
          setup a a real-time kernel with ros) for the communication
          with the robot.<br>
          <code>
            cd $HOME<br>
            mkdir -p catkin_ws/src<br>
            cd catkin_ws<br>
            source /opt/ros/&ltros-distro&gt/setup.sh<br>
            catkin_init_workspace src<br>
            git clone --recursive https://github.com/frankaemika/franka_ros
            src/franka_ros<br>
            rosdep install --from-paths src --ignore-src --rosdistro
            <ros-distro> -y --skip-keys libfranka<br>
            source devel/setup.sh<br>
          </code>
          </p>
          <p>
          Finally, we can install the custom controller used by the Platonics in
          the robothon 2023 using the following:<br>
          <code>
            roscd franka_ros<br>
            git clone
            git@github.com:platonics-delft/franka_control_robothon_challenge.git
            <br>
            roscd<br>
            cd ..<br>
            source /opt/ros/&ltros-distro&gt/setup.bash<br>
            catkin_make -DMAKE_BUILD_TYPE=Release
            -DFranka_DIR:PATH=~/libfranka/build<br>
          </code>
          </p>
          <p>
          You can now run the controller using<br>
          <code>roslaunch franka_human_friendly_controllers\<br>
            cartesian_variable_impedance_controller.launch robot_ip:=ROBOT_IP
            load_gripper:=True<br>
          </code>
          </p>
          <h3><a href="https://github.com/platonics-delft/robothon23_vision">robothon23_vision</a></h3>
          <p>
          The vision components are a pure python package and can be installed
          using <em>pip</em>. This will automatically install the dependencies,
          i.e. <code>opencv-python, scipy, numpy</code>.<br>
          <code class="language-bash">
            git clone git@github.com:platonics-delft/robothon23_vision.git<br>
            cd robothon23_vision<br>
            pip install .<br>
          </code>
          </p>
          <h3><a
              href="https://github.com/platonics-delft/robothon23_gui">robothon23_gui</a></h3>
          <p>
          The gui can be installed using:<br>
          <code class="language-bash">
            git clone git@github.com:platonics-delft/robothon23_gui.git<br>
            cd robothon23_gui<br>
            pip install .<br>
          </code>
          </p>
          <h3><a
              href="https://github.com/platonics-delft/robothon_23_platonics">ros
              packages</a></h3>
          <p>
          Finally, all the ros related packages are located here. For example,
          the trajectory manager and the active box_localization.<br>
          <code class="language-bash">
            sudo apt install ros-&ltros-distro&gt-realsense-ros<br>
            cd $HOME/catkin_ws<br>
            source devel/setup.bash<br>
            cd src<br>
            git clone git@github.com:platonics-delft/robotthon_23_platonics.git#debug-camera-feedback<br>
            roscd<br>
            cd ..<br>
            catkin_make<br>
            source devel/setup.bash<br>
          </code>
          </p>
        </div>
        <h2 class="title is-3">Getting started: Demonstrate, Correct, Generalize, Execute</h2>

        <h2 class="title is-2">Record the template of the object in the current robot view </h2>
        <p>
           To record and save the template that is going to be used to localized the object follow the instructions here: 
           <a href="https://github.com/platonics-delft/robothon23_gui/blob/main/README.md"> here</a>
     
         </p>    

        <div class="content has-text-justified">
          First of all we need to start the controller with
          <p>
          <code class="language-bash">
            roslaunch frank_human_friendly_controllers cartesian_variable_impedance_controller.launch robot_ip:=ROBOT_IP
          </code>.
          </p>
          <p>
          Then we start the camera with
          <code class="language-bash">
            roslaunch trajectory_manager controller_camera.launch 
          </code>.
          </p> 
          <p>
          You can now record a demonstration with:
          </p>  
          <p>  
          <code class="language-bash">
          python3 manipulation_tasks_panda/trajectory_manager/recording_trajectory.py "name skill"
          </code>.
          </p>  
          <p>
          For executing the skill you can run 
          </p>  
          <p>  
          <code class="language-bash">
          python3 manipulation_tasks_panda/trajectory_manager/playback_trajectory.py "name skill"
          </code>.
          </p>  
          <p>
          To run all the skill in a row, you can modify the manipulation_tasks_panda/trajectory_manager/playall.py according to the name of your skills. The you can run 
          </p>  
          <p>
           <code class="language-bash">
          python3 manipulation_tasks_panda/trajectory_manager/playall.py 0
          </code>
          </p>  
          <p>  
          if you don't want to have the active localizer or 
          </p>  
          <p> 
          <code class="language-bash">
          python3 manipulation_tasks_panda/trajectory_manager/playall.py 1
          </code>, otherwise.
           <p> 
          The second command requires that you also launch the localization service with 
          </p>  
          <p>  
          <code class="language-bash">
          roslaunch box_localization box_localization.launch 
          </code>
          </p>  
          In this second case, the robot will first localize the object to match the template given during demonstration, trasform the skill in the new reference frame and then execute it. 
        </div>
        <p>
          During demonstration or during execution, it is possible to give feedback to the learned skill using the computer keybord. 
        <ul>
          <li>Press <b> e </b>  to stop the recording </li>
        </ul>
        <p>
        <b> Gripper commands:</b>
        <ul>
          <li>Press <b> c</b> to close the gripper </li>
          <li>Press <b> o </b> to open the gripper </li>
        </ul>
      </p>
      <p>  
        <b> Camera feedback: </b>
        <ul>
          <li>Press <b> k </b> to add a label to enable the camera feedback from that point in on </li>
          <li>Press <b> l</b> to add a label to disable the camera feedback from that point in on</li>
        </ul>
      </p>
      <p>
       <b> Haptic feedback: </b>
        <ul>
          <li>Press <b> z</b> to add a label to enable the haptic feedback from that point in on</li>
          <li>Press <b> x</b> to add a label to enable the haptic feedback from that point in on</li>
        </ul>
      </p>
        <p>  
      The motivation of explictly labeling or disenabling the haptics and local camera feedback is because during a long trajectory the user can explictly teach the robot to use or not that skill. 
      For example, it makes sense to have the haptic feedback only active when doing insertion tasks, such that the robot will not start spiraling when external forces are perceived but they are not due to critical insertion tasks.   
      It is worth noticing, that if, no insertion task is perfomed, in case of large force, the robot would temporarly stop the execution of the motion, until the distrubance is gone. 
      </p>
      
      <p>
       <b> Directional feedback </b>
        <ul>
          <li>Press <b> w</b> to locally slightly shift the trajectory in positive x in robot frame </li>
          <li>Press <b> s</b> to locally slightly shift the trajectory in negative x in robot frame</li>
          <li>Press <b> a</b> to locally slightly shift the trajectory in positive y in robot frame</li>
          <li>Press <b> d</b> to locally slightly shift the trajectory in negative y in robot frame</li>
          <li>Press <b> u</b> to locally slightly shift the trajectory in positive z in robot frame</li>
          <li>Press <b> j</b> to locally slightly shift the trajectory in negative z in robot frame</li>
        </ul>
      <p>
      This feedback is useful when, after the demonstration, the robot will, for example, not press the button hard enough. However, by pressing j, the trajectory is locally modified and when executing again, the robot will apply enough force.    
      </p> 
      
      <p>
      At the end of every play back, the computer will ask if to overwrite the old trajectory or not. If the changes are accepted, the robot will remember the feedback in the next executions. 
      </p>

    </div>
    </div>
    <!--/ Software. -->
    <!--Extras. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Extras</h2>
        <div class="content has-text-justified">
          <p>
            The additional material, such as the website's source code and <thead>
            presentation can be found in the same github organizition
            <a href="https://github.com/orgs/platonics-delft">
               Platonics Delft</a>:
            <ol>
              <li><a href="https://github.com/platonics-delft/platonics-delft.github.io">Website source code</a></li>
              <li><a href="https://github.com/platonics-delft/presentation_robothon_2023">Presentation</a></li>
            </ol> 
          </p>
          <p>
          </p>
        </div>
      </div>
    </div>
    <!--/ Extras. -->
</section>




</body>
</html>
