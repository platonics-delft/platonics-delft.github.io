<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Platonics Robothon 2023</title>

		<meta name="description" content="Cracking robotic challenges in a generic manner.">
		<meta name="author" content="Max, Mariano, Giovanni, Chaid">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="node_modules/reveal.js/dist/reset.css">
		<link rel="stylesheet" href="node_modules/reveal.js/dist/reveal.css">
		<link rel="stylesheet" href="node_modules/reveal.js/dist/theme/solarized.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="node_modules/reveal.js/plugin/highlight/monokai.css">
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
          <table>
            <tr>
              <td>
                <img width='400px', src="assets/tudelft.png" alt="">
              </td>
          </table>
					<h1>Platonics</h1>
          <h2>Robothon 2023</h2>
          <h4>Solving fine manipulation tasks generically</h4>
					<p>
						<small>
              Giovanni Franzese, Mariano Ramirez, Chadi Salmi, Max Spahn<br>
              Code @ <a href="https://github.com/platonics-delft">platonics-delft</a>
            </small>
					</p>
          <img src="assets/platonics_website.png" height='200px' alt="https://platonics-delft.github.io/">
          <aside class="notes">
            Hello everybody, my name is Max, and I am a member of the Paltonics
            team together with Giovanni, Mariano and Chadi, we are a team of
            researchers from TU Delft. We happy to present to you our approach
            based on learning from human demonstration to the robothon 2023
            Grand challenge. Our goal is to facilitate robotic deployment
            so that humans can be relieved from physically deamanding and dull
            tasks. The human is central in our approach as we will layout in
            this short presentation.
          </aside>
				</section>

        <section>
          <h3>Motivation</h3>
          <ul>
            <li><em>57.4 Million Metric Tonnes in 2021</em>[1]</li>
            <li>repairing is even more valuable [2]</li>
            <li>Bring your own device task, image ???</li>
          </ul>
          <img src="assets/ewaste_2019_nature.png" height='400px' alt="E-Waste 2019">[3]
          <p id="cite1" style="bottom: 0; text-align: left"><small>
            [1] <a href="https://theroundup.org/global-e-waste-statistics/">https://theroundup.org/global-e-waste-statistics/</a><br>
            [2] <a href="https://www.ifixit.com/Info/environment">https://www.ifixit.com/Info/environment</a><br>
            [3] M. Eisenstein: "Short-circuting the electronic-waste crisis", Nature 2022
          </small></p>
          <aside class="notes">
            Recycling of electronic waste is an important challenge in this
            context, deassambling and recycling of it is needed to keep our
            environment healthy and livable for future generations. Also,
            current approaches to recycling is often harmful to the health of
            individuals in the global South.
            In 2021, the world population produced 57.4 Million Metric Tonnes of 
            electronic waste. Most of which is not recycled and ends up
            polutting our environment. While the causes for electronic waste are
            manifold, solution to the reduction do not only include recycling
            but also repairing, because many times, electronic devices are not
            perse broken but it is simply not worth repairing them. Cheap
            robotic solutions could help us extend the life-cycles of electronic
            devices and simplify the recycling process. As both tasks require
            very fine manipulation skills, we are convinced that robots must be
            taught by human to leverage all the implicit knowledge that we, as
            humans, have learned in our long history.
          </aside>

        </section>

        <section>
          <h3> Requiremnts</h3>
          <section>
            <ul>
              <li>[REQ 1] working in proximity of humans</li>
              <li>[REQ 2] robust to uncertainty</li>
              <ul>
                <li>humans</li>
                <li>localization</li>
                <li>control</li>
              </ul>
              <li>[REQ 3] for non-expert users</li>
              <li>[REQ 4] high accuracy</li>
              <li>[REQ 5] generalizable</li>
            </ul>
            <aside class='notes'>
              Based on the challenges given by the problem of ewaste reduction,
              we can formulate 5 requirements:
              1) Humans are central to our approach, so the robot must be safe
              around them.
              2) Putting robots outside of metal fences introduces
              uncertainities, caused by humans, localization or robotic
              controllers. A proposed approach must be robust under this
              uncertainties.
              3) There are not enough engineers to program all the robots in
              numbers we
              need to really reduce electronic waste, so we want to make an
              approach that is accessible to non-expert users.
              4) Fine manipulation requires very accurate motion of the robot
              5) As every electronic device is different, a solution must 
              quickly transfer to new devices.
            </aside>
          </section>
        </section>

        <section>
          <h3> Platonics Features</h3>
          <section>
            <h4>Resources</h4>
            <ul>
              <li>Franka Emika Panda, two-finger gripper [REQ 1,2]</li>
              <li>Intel Realsense D435</li>
              <li>3D-printed mount camera, and finger tip</li>
              <li>Intel NUC i7 computer</li>
            </ul>
            <table>
              <tr>
                <td>
                  <video src="assets/compliance.mp4" height="250px" loop autoPlay muted></video>
                </td>
              </tr>
            </table>
            <aside class="notes">
              Having discussed the requirements, we want to briefly introduce
              the core features of the platonics solution and convince you that
              learning from human demonstration is the way to go to reduce
              electronic waste and improve the end-of-life treatment.

              First, we use a Franka Emika Panda arm with the default two-finger
              gripper and a 3D printed finger tip. Together with the commonly
              used
              realsense d435 and a average Intel NUC i7, our setup is complete.
              Note the simplicity of our setup, because we believe that the
              hardware setup must be affordable and simple to obtain to be
              really competetive.
            </aside>
          </section>
          <section>
            <h4>Humans</h4>
            <ul>
              <li>learn policy from <em>single</em> demonstration [REQ 5]</li>
              <li>human teaches implicit knowledge [REQ 3]</li>
            </ul>
            <table>
              <tr>
                <td>
                  <video src="assets/insertion_demo.mp4" height="300px" loop autoPlay muted></video>
                </td>
                <td>
                  <video src="assets/boyd_demo_ram.mp4" height="300px" loop autoPlay muted></video>
                </td>
              </tr>
            </table>
            <p id="cite1" style="bottom: 0; text-align: left"><small>
              [1] C. Celemin et al.: "Interactive imitation learning in
              robotics: A survey"<br>
              [2] H. Ravichandar et al.: "Recent advances in robot learning from demonstration"
            </small></p>
            <aside class="notes">
              Humans are central to our approach so they are explicitely listed
              as a feature. Specifiaclly, our solution is centered on 
              learning
              demonstrations from humans, specifically non-expert robot
              programmers. Any human with an understanding of the task
              can record a trajectory that fulfills the task.

              I want to pause here for a second to let the approach sink in...
              You understand it correctly, the human guides the robot to do a
              task that the human can already perform quite well. By doing so,
              implicit knowledge, that is hard to express in numbers is being
              passed to the robot. With that approach, even very cumbersome
              motions such as the opening of a hinged door, or the wrapping of
              cable can be easily programmed, or as we call it taught.
            </aside>
          </section>
          <section>
            <h4>Controller</h4>
            <ul>
              <li>Cartesian impedence control [REQ 4]</li>
              <li>Exploiting redundancy</li>
              <li>Robustness in manipulation tasks</li> 
            </ul>
            <aside class="notes">
              The controller is a simple impedence controller, to allow us to
              have a safe motion that can be interfered by the human at all
              time. Note that the controller operates in Cartesian Space, so
              that redundancy of the robotic arm can be exploited while
              executing Cartesian trajectories taught by humans.
            </aside>
          </section>
          <section>
            <h4>Policy Transfer</h4>
            <ul>
              <li>Scale-Invariant-Feature-Transformation</li>
              <li>Requires <em>no</em> labeling [REQ 5]</li>
              <li>transform recorded trajectories</li>
              <img src="assets/template_matching.png" height="200px" alt="Template Matching
              BOYD">
              <br>
            </ul>
            <aside class='notes'>
              To generalize to different positions of the object to interact
              with, the trajectories must be adapted based on camera feedback. 
              We use a template matching algorithm, called SIFT, together with
              active localization. The robot iteratively moves so that the
              sensed image matches the template for which the skill were taught.

              Once, the transformation between teaching and curren situation is
              determined with sufficient accuracy, the Cartesian trajectories
              can be transformed and execued.
            </aside>
          </section>
          <section>
            <h4>Feedback</h4>
            <ul>
              <li>Haptic feedback (spiralling, stabbing) [REQ 2]</li>
              <li>Visual feedback, online feature matching [REQ 2]</li>
            </ul>
            <table>
              <tr>
                <td>
                  <video src="assets/spiraling.mp4" height="160px" loop autoPlay muted></video>
                </td>
                <td>
                  <video src="assets/sift_cam_feedback.mp4" height="160px" loop autoPlay muted></video>
                </td>
              </tr>
            </table>
            <table>
              <tr>
                <td>
                </td>
                <td>
                  <video src="assets/stabbing.mp4" height="160px" loop autoPlay muted></video>
                </td>
              </tr>
            </table>

          <aside class="notes">
            However, pure playpack is not enough so alongside Cartesian 
            trajectories, external features of trajectories are saved, such as
            visuals and haptics. Online, the robot verifies whether images match
            the recording and actively corrects for it, it also continuously
            verifies haptic feedback to sense whether something is going wrong.
            In this case, the robot retracts and tries again.

            Additionally, humans can still adjust trajectories during the firs
            execution to compensate for inaccuracies in their recording.
          </aside>

          </section>
        </section>

        <section>
          <h3>Platonics Walk through</h3>
            <ol>
              <li>Record a RGB-Template</li>
              <li>Record trajectories and verify their success</li>
              <li>Select order of tasks</li>
              <li>Execute sequence of tasks</li>
            </ol>
            <table>
              <tr>
                <td>
                  <video src="assets/gui_template_creation.mp4" height="250px" loop autoPlay muted></video>
                </td>
                <td>
                  <video src="assets/gui_record.mp4" height="250px" loop autoPlay muted></video>
                </td>
              </tr>
            </table>
            <table>
              <tr>
                <td>
                  <video src="assets/gui_playback.mp4" height="250px" loop autoPlay muted></video>
                </td>
                <td>
                  <video src="assets/full_run.mp4" height="250px" loop autoPlay muted></video>
                </td>
              </tr>
            </table>
            <aside class='notes'>
              As you now know about the principle concept of the platonics
              solution, the walk through of our solution is straight-forward. 
              1) Record a template of the device you want to recycle or repair.
              2) Teach the robot an individual skills by guiding it though the
              steps
              3) Select the order in which the skills should be executed
              4) Execute the task using active localization
            </aside>
        </section>

        <section>
          <section>
            <h4>BYOD Task</h4>
            <ul>
              <li>No modification required [REQ 5]</li>
            </ul>
            <br>
            <table>
              <tr>
                <td><video src="assets/byod_battery_demo.mp4" height="250px" loop autoPlay muted></video></td>
                <td><video src="assets/byod_harddrive_demo.mp4" height="250px" loop autoPlay muted></video></td>
                <td><video src="assets/byod_probing_demo.mp4" height="250px" loop autoPlay muted></video></td>
              </tr>
              <tr>
                <td><video src="assets/byod_battery_playback_changed.mp4" height="250px" loop autoPlay muted></video></td>
                <td><video src="assets/byod_harddrive_playback_fixed.mp4" height="250px" loop autoPlay muted></video></td>
                <td><video src="assets/byod_probing_playback_fixed.mp4" height="250px" loop autoPlay muted></video></td>
              </tr>
            </table>
            <aside class="notes">
              For the bring your own device task, we decide to unmount an old
              laptop. The process was very simple because our solution is
              generalizable and easy to deploy. Here you see some videos of us
              teaching the to extract the RAM, the hard drive and the
              battery...
            </aside>
          </section>
        </section>


        <section>
          <h3>Why Platonics?</h3>
          <ul>
            <li>leverage human demonstration</li>
            <li>platonics solutions are generic</li>
            <li>easy to transfer and implement</li>
          </ul>
          <br>
          <img src="assets/platonics_website.png" height='200px' alt="https://platonics-delft.github.io/">
          <aside class="notes">
            So, why the platonics? Hopefully, you are convinced by now, but to
            recap briefly.
            We leverage humans demonstration, because programming robots using
            numbers is tedious and not competetive in the long run.
            With that philosophy, we can generalize to almost every task that is
            physically feasible for the robot.
            Not only is our approach safe in the proximity of humans, it is also
            cheap and easy to implement and deploy.

            Thanks for having us let participate in the Grand Challenge and for
            you attention. We enjoyed the last month very much.
            Scan the QR-Code to learn more about the approach, details about our
            implementation and resources.
          </aside>
        </section>


			</div>

		</div>

		<script src="node_modules/reveal.js/dist/reveal.js"></script>
		<script src="node_modules/reveal.js/plugin/zoom/zoom.js"></script>
		<script src="node_modules/reveal.js/plugin/math/math.js"></script>
		<script src="node_modules/reveal.js/plugin/notes/notes.js"></script>
		<script src="node_modules/reveal.js/plugin/search/search.js"></script>
		<script src="node_modules/reveal.js/plugin/markdown/markdown.js"></script>
		<script src="node_modules/reveal.js/plugin/highlight/highlight.js"></script>
		<script>

			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
				controls: true,
				progress: true,
        slideNumber: true,
				center: true,
				hash: true,

				// Learn about plugins: https://revealjs.com/node_modules/reveal.js/plugins/
				plugins: [ RevealMath.KaTeX, RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight ]
			});

		</script>

	</body>
</html>
